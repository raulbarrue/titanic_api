{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.2 64-bit ('azure-training': conda)",
   "display_name": "Python 3.8.2 64-bit ('azure-training': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f6cc2971799897eb0ca02973c4931246881cb407ba037d98f8a579c2cbf5eea6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "titanic-ws workspace loaded.\nworkspaceblobstore datastore loaded.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace \n",
    "\n",
    "ws = Workspace(subscription_id=\"fc1c5e68-95c2-4bce-9ff9-5bd8442fb921\", resource_group=\"titanic-api\", workspace_name=\"titanic-ws\")\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "print(ws.name, \"workspace loaded.\")\n",
    "print(default_ds.name, \"datastore loaded.\")"
   ]
  },
  {
   "source": [
    "# Modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import titanic_functions as tfunc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df2 = pd.read_csv(\"../dataset/titanic_data.csv\")\n",
    "df2 = tfunc.clean_df(df2)\n",
    "\n",
    "X = df2.drop(\"survived\", axis=1)\n",
    "y = df2[\"survived\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "Xsc = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in two equal parts\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xsc, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "gnb = GaussianNB()\n",
    "rfc = RandomForestClassifier()\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = gnb.fit(X_train, y_train)\n",
    "rfc = rfc.fit(X_train, y_train)\n",
    "lr = lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gnb = gnb.predict(X_test)\n",
    "pred_rfc = rfc.predict(X_test)\n",
    "pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gaussian NB:\n\tAccuracy: 0.7023\n\tPrecision: 0.5897\n\tRecall: 0.8679\n\tF1-Score: 0.7023\n\nRandom Forest Classifier:\n\tAccuracy: 0.7863\n\tPrecision: 0.7451\n\tRecall: 0.717\n\tF1-Score: 0.7308\n\nLogistic Regression:\n\tAccuracy: 0.8282\n\tPrecision: 0.7699\n\tRecall: 0.8208\n\tF1-Score: 0.7945\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score, precision_score, recall_score, roc_curve, roc_auc_score, classification_report, f1_score\n",
    "import numpy as np\n",
    "\n",
    "models = {\"Gaussian NB\": pred_gnb, \"Random Forest Classifier\": pred_rfc, \"Logistic Regression\": pred_lr}\n",
    "metrics = {}\n",
    "\n",
    "for name, pred in models.items():\n",
    "    print(name+\":\")\n",
    "\n",
    "    print(\"\\tAccuracy:\", np.round(accuracy_score(y_test, pred),4))\n",
    "    print(\"\\tPrecision:\", np.round(precision_score(y_test, pred),4))\n",
    "    print(\"\\tRecall:\", np.round(recall_score(y_test, pred),4))\n",
    "    print(\"\\tF1-Score:\", np.round(f1_score(y_test, pred),4))\n",
    "    print()\n",
    "\n",
    "    m = {\"Accuracy:\": np.round(accuracy_score(y_test, pred),4),\n",
    "        \"Precision:\": np.round(precision_score(y_test, pred),4),\n",
    "        \"Recall:\": np.round(recall_score(y_test, pred),4),\n",
    "        \"F1-Score:\": np.round(f1_score(y_test, pred),4)}\n",
    "    metrics[name] = m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Uploading an estimated of 2 files\n",
      "Uploading model_folder/titanic-api-model.pkl\n",
      "Uploaded model_folder/titanic-api-model.pkl, 1 files out of an estimated total of 2\n",
      "Uploading model_folder/titanic-api-scaler.pkl\n",
      "Uploaded model_folder/titanic-api-scaler.pkl, 2 files out of an estimated total of 2\n",
      "Uploaded 2 files\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_ea0fee9a0a034a629ca616c5852d0d86"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "best_model = lr\n",
    "best_model_name = \"Logistic Regression\"\n",
    "best_model_metrics = metrics[best_model_name]\n",
    "\n",
    "os.makedirs(\"model_folder\", exist_ok=True)\n",
    "joblib.dump(best_model, \"model_folder/titanic-api-model.pkl\")\n",
    "joblib.dump(scaler, \"model_folder/titanic-api-scaler.pkl\")\n",
    "\n",
    "default_ds.upload_files([\"model_folder/titanic-api-model.pkl\", \"model_folder/titanic-api-scaler.pkl\"], target_path=\"model_folder\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Registering model titanic-api-model\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='titanic-ws', subscription_id='fc1c5e68-95c2-4bce-9ff9-5bd8442fb921', resource_group='titanic-api'), name=titanic-api-model, id=titanic-api-model:10, version=10, tags={'Accuracy:': '0.8282', 'Precision:': '0.7699', 'Recall:': '0.8208', 'F1-Score:': '0.7945'}, properties={})"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "Model.register(ws, \"model_folder/titanic-api-model.pkl\", \"titanic-api-model\", tags=best_model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Registering model titanic-api-scaler\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='titanic-ws', subscription_id='fc1c5e68-95c2-4bce-9ff9-5bd8442fb921', resource_group='titanic-api'), name=titanic-api-scaler, id=titanic-api-scaler:7, version=7, tags={'Scaler': 'Titanic Input Scaler'}, properties={})"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "Model.register(ws, \"model_folder/scaler.pkl\", \"titanic-api-scaler\", tags={\"Scaler\":\"Titanic Input Scaler\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model.get_model_path(\"titanic-api-model\")\n",
    "# scaler = Model.get_model_path(\"titanic-api-scaler\")"
   ]
  },
  {
   "source": [
    "# DEPLOYMENT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting model_folder/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_folder/score.py\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    global scaler\n",
    "    model_path = './model_folder/titanic-api-model.pkl'\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    scaler_path = './model_folder/titanic-api-scaler.pkl'\n",
    "    scaler = joblib.load(scaler_path)\n",
    "\n",
    "###### POWER BI ######\n",
    "\n",
    "# providing 3 sample inputs for schema generation\n",
    "standard_sample_input = StandardPythonParameterType([1, \"Raul\", \"male\", 25, 0, 0, 300, \"C32\", \"S\"])\n",
    "\n",
    "# This is a nested input sample, any item wrapped by `ParameterType` will be described by schema\n",
    "sample_input = StandardPythonParameterType({'input1': standard_sample_input})\n",
    "\n",
    "#sample_global_parameters = StandardPythonParameterType(1.0) #this is optional\n",
    "sample_output = StandardPythonParameterType([1.0])\n",
    "\n",
    "@input_schema('inputs', sample_input)\n",
    "#@input_schema('global_parameters', sample_global_parameters) #this is optional\n",
    "@output_schema(sample_output)\n",
    "\n",
    "\n",
    "######################\n",
    "\n",
    "\n",
    "def run(data):\n",
    "\n",
    "    #ws = Workspace(subscription_id=\"fc1c5e68-95c2-4bce-9ff9-5bd8442fb921\", resource_group=\"titanic-api\", workspace_name=\"titanic-ws\")\n",
    "\n",
    "    try:\n",
    "        data = json.loads(data)['data']\n",
    "        data = scaler.transform([data])\n",
    "        result = model.predict(data)\n",
    "        # You can return any data type, as long as it is JSON serializable.\n",
    "        return result.tolist()\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Deployment\n"
     ]
    }
   ],
   "source": [
    "# Deployment Env\n",
    "\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create an environment and add conda dependencies to it\n",
    "myenv = Environment(name=\"myenv\")\n",
    "# Enable Docker based environment\n",
    "myenv.docker.enabled = True\n",
    "# Build conda dependencies\n",
    "myenv.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'],\n",
    "                                                           pip_packages=['azureml-defaults'])\n",
    "                                                        \n",
    "print(\"Deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "# Deployment config\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "# from azureml.core import Model \n",
    "\n",
    "# model = ws.models[\"titanic-api-model\"]\n",
    "\n",
    "inference_config = InferenceConfig(source_directory=\"model_folder\",\n",
    "                                   entry_script=\"score.py\",\n",
    "                                   environment=myenv)\n",
    "\n",
    "# deploy_config = AciWebservice.deploy_configuration(cpu_cores=0.1, memory_gb=0.5)\n",
    "\n",
    "# service_name = \"titanic-api\"\n",
    "\n",
    "# service = Model.deploy(ws, service_name, [model], inference_config, deploy_config)\n",
    "# service.wait_for_deployment(True)\n",
    "# print(service.state)\n",
    "\n",
    "service = AciWebservice(ws, \"titanic-api\")\n",
    "service.update(inference_config=inference_config)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "http://a278d689-be85-4c99-a512-1b715ae287db.uksouth.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "# TEST ENDPOINT\n",
    "endpoint = service.scoring_uri\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "\n",
    "x_new = list(df2.drop(\"survived\",axis=1).iloc[i].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "input_json = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Set the content type\n",
    "headers = { 'Content-Type':'application/json' }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted: 1\nActual: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted:\", requests.post(endpoint, input_json, headers = headers).json()[0])\n",
    "print(\"Actual:\", df2[\"survived\"].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "import titanic_functions as tfunc\n",
    "\n",
    "data=[[1, \"Raul\", \"male\", 25, 0, 0, 300, \"C32\", \"S\"]]\n",
    "\n",
    "data=list(tfunc.clean_df(data).values[0])\n",
    "\n",
    "input_json = json.dumps({\"data\": data})\n",
    "headers = { 'Content-Type':'application/json' }\n",
    "\n",
    "requests.post(endpoint, input_json, headers = headers).json()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.0, 25.0, 0.0, 0.0, 300.0, 1.0, 300.0, 1.0, 25.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}